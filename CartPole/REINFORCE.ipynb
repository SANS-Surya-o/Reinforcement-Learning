{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f8c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Define the Policy Network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "# REINFORCE Agent\n",
    "class ReinforceAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.01, gamma=0.99):\n",
    "        self.policy = Policy(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.policy(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "\n",
    "    def finish_episode(self):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = deque()\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.appendleft(R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-6) # Normalize returns\n",
    "\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        del self.rewards[:]\n",
    "        del self.saved_log_probs[:]\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "def main():\n",
    "    # Environment setup\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Agent setup\n",
    "    agent = ReinforceAgent(state_size, action_size)\n",
    "\n",
    "    # Training parameters\n",
    "    n_episodes = 1500\n",
    "    print_every = 100\n",
    "\n",
    "    # Logging\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    losses = []\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        for t in range(1000):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        scores_deque.append(episode_reward)\n",
    "        scores.append(episode_reward)\n",
    "        loss = agent.finish_episode()\n",
    "        losses.append(loss)\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
    "        \n",
    "        if np.mean(scores_deque) >= 475.0:\n",
    "            print(f'\\nEnvironment solved in {i_episode-100} episodes!\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
    "            break\n",
    "\n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "    # Plot Training Scores and Rolling Average\n",
    "    ax1.plot(np.arange(1, len(scores) + 1), scores, label='Training Scores')\n",
    "    rolling_avg = [np.mean(scores[k:k+100]) for k in range(len(scores)-100)]\n",
    "    ax1.plot(np.arange(100, len(scores)), rolling_avg, color='r', label='Rolling Average Score (100 episodes)')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Training Scores and Rolling Average')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Average Loss per Episode\n",
    "    ax2.plot(np.arange(1, len(losses) + 1), losses, color='orange', label='Average Loss per Episode')\n",
    "    ax2.set_xlabel('Episode #')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Average Loss per Episode')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
